+++
date = 2019-09-25
draft = true
tags = ["Dirichlet process", "Gibbs sampling", "machine learning", "Bayesian statistics"]
title = "Writing Gibbs Samplers for clustering binary data using Dirichlet Processes"
math = true
+++

In the previous entry of what has evidently become a series on modelling
binary mixtures with Dirichlet Processes ([part 1 discussed using
`pymc3`](https://www.stuartlacy.co.uk/2019/02/11/modelling-bernoulli-mixture-models-with-dirichlet-processes-in-pymc/)
and [part 2 detailed writing custom Gibbs samplers](https://stuartlacy.co.uk/2019/09/09/writing-gibbs-samplers-for-clustering-binary-data-using-dirichlet-processes/)), I ended by stating that I'd like to look into writing a Gibbs sampler using the stick-breaking formulation of the Dirichlet Process, in contrast to the Chinese Restaurant Process (CRP) version I'd just implemented.

Actually coding this up this was rather straight forward and took less time than I expected, but I found the differences and similarities between these two same ways of expressing the same mathematical model interesting enough for a post of its own.

R package
---------

As mentioned last time, all the code used in these posts is 
[available on Github](https://github.com/stulacy/bmm-mcmc) as an R package
and can be installed using `devtools` (not tested on Windows). However, do
note that since this package is primarily written for research purposes,
it’s possible that the API used here is outdated in the future.
I do intend on cleaning it up to provide a more
consistent interface across all the samplers, as well as 
refactoring the largely duplicated backend code.
For posterity, this post was written with the code at [this
commit](https://github.com/stulacy/bmm-mcmc/commit/8a172095075d60c5d104db3ef15d2e8ca115051b).

    devtools::install_github('stulacy/bmm-mcmc')

Dataset
-------

As with the last post, I’ll use the dataset with 1,000 observations of 5
binary variables that are formed from 3 classes. All variables have a
different rate in each class - a rather nice simplification that will
never occur in real world data.

    library(bmmmcmc)
    library(tidyverse)
    data(K3_N1000_P5)

The code used to simulate this data is shown below (also available in
`R/simulate_data.R`).

    set.seed(17)
    N <- 1000
    P <- 5

    theta_actual <- matrix(c(0.7, 0.8, 0.2, 0.1, 0.1,
                             0.3, 0.5, 0.9, 0.8, 0.6,
                             0.1, 0.2, 0.5, 0.4, 0.9),
                           nrow=3, ncol=5, byrow = T)
    cluster_ratios <- c(0.6, 0.2, 0.2)

    mat_list <- lapply(seq_along(cluster_ratios), function(i) {
                n <- round(N * cluster_ratios[i])
                sapply(theta_actual[i,], function(p) rbinom(n, 1, p))
    })

    mat <- do.call('rbind', mat_list)
    saveRDS(mat, "data/K3_N1000_P5_clean.rds")

Dirichlet Process
-----------------

Let’s quickly recap Dirichlet Processes as I’ve shied away from doing it
so far, mostly because I’ve only recently come
to get a good feel for them myself. 

The standard answer is that they are a distribution of distributions, 
so that each draw $G$ is in itself a distribution.
Formally, they are parameterised by a base distribution $G\_0$
and a concentration parameter $\alpha$.

$G \sim DP(\alpha, G\_0)$

Does that make sense? 
If you're like me then probably not immediately!

What helped me them really click for me was actually using them in
mixture modelling, and in particular
[Wikipedia’s derivation of their relationship to a finite
mixture
model](https://en.wikipedia.org/wiki/Dirichlet_process#Use_in_Dirichlet_mixture_models).

Starting with a standard finite mixture model, the data
$y\_i$ is distributed according to a known distribution
$F(.)$ where each cluster has its own parameters $\theta\_c$.

$$y\_i\ |\ c\_i,\theta \sim F(\theta\_{c\_i})$$

The cluster labels themselves are distributed according to a categorical
distribution with probabilities given by $\pi$.

$$c\_i\ |\ \pi \sim \text{Cat}\_K(\pi)$$

With the prior on $\pi$ being the conjugate Dirichlet, with the
concentration parameter $\alpha$.

$$\pi\ |\ \alpha \sim \text{Dirichlet}\_{K}(\frac{\alpha}{K})$$

Finally, the cluster components $\theta$ are distributed according to a known
distribution $G\_0$.

$$\theta\_c \sim G\_0$$

In other words the above probability model suggests that each observation is 
assigned a cluster and then their data is drawn from $F$ using
that cluster's $\theta\_c$.
Instead, we can rewrite the same model so that it can now be thought of
as each observation having their own parameter $\bar{θ}\_i$,
which has been drawn from a discrete distribution $G$.
Where $\delta\_{\theta\_k}$ is the indicator function
$G$ is expressed as the sum of

$$y\_i\ |\ \bar{\theta}\_i \sim F(\bar{\theta}\_i)$$

$$\bar{\theta}\_i \sim G = \sum\_{k=1}^{K} \pi\_k \delta\_{\theta\_k}(\bar{\theta}\_i)$$

$$\pi\ |\ \alpha \sim \text{Dirichlet}\_{K}(\frac{\alpha}{K})$$
$$\theta\_c \sim G\_0$$

Now, when $K$ is infinite, $G$ becomes the sum to infinity above, which
is exactly a Dirichlet Process, i.e.:

$$y\_i\ |\ θ̄\_i \sim F(\theta\_i)$$

$$\theta\_i \sim G$$

$$G \sim DP(\alpha, G\_0)$$

So here $G\_0$ is the prior on the data parameters, for our
binary data this would be the Beta distribution being as it is the
conjugate pair of the Bernoulli. However, note that it is actually the
data parameters that have their distribution according to a DP, often it
is said that the Dirichlet Process is a prior on the number of
components $K$, which isn’t strictly true.

Chinese Restaurant Process
--------------------------

There are several alternative ways of visualising how Dirichlet Process
actually works, one of the most well known being the Chinese Restaurant
Process (CRP). Under this model, a person enters a restaurant with a
given number of tables where people are already sat (the current number
of clusters in the model $K$). They either have the choice of sitting at
an existing table, ideally with people they are most similar too, or
sitting down at their own table.

The probability of sitting down at an existing table is given by the
following, where $N\_nc$ is the number of people already
sat at Table $c$ and $\alpha$ is a tuning parameter, where high values make
it more likely that a person sets up their own table.

$$P(c\_{i} = c\ |\ c\_{-i}) = \frac{N\_{-nc}}{N-1+\alpha}$$

The probability of sitting at a new table is given by the following,
note now $α$ is on the numerator so a high value makes a new table more
likely.

$$P(c\_{i} = c^{\*}\ |\ c\_{-i}) = \frac{\alpha}{N-1+\alpha}$$

Extending these probabilities to take into account the proposed mixture
model structure of the data results in Eq 3.6 of [(Neal
2000)](http://www.stat.columbia.edu/npbayes/papers/neal_sampling.pdf).

$$P(c\_{i} = c\ |\ c\_{-i}, y\_{i}) = \frac{N\_{-nc}}{N-1+\alpha}\prod\_{d=1}^{D} \int F(y\_i, \theta)dH\_{-i, c}(\theta)$$

$$P(c\_{i} = c^{\*}\ |\ c\_{-i}, y\_{i}) = \frac{\alpha}{N-1+\alpha}\prod\_{d=1}^{D} \int F(y\_i, \theta)dG\_{0}(\theta)$$

[van der Maaten (Van Der Maaten,
n.d.)](http://lvdmaaten.github.io/publications/papers/TR_BMBD_2010.pdf)
provides these values for the case where $F$(.) is the Bernoulli
distribution as in the case of our data, and $G$<sub>0</sub> is
Beta($β$, $γ$).

$$P(c\_{i} = c\ |\ c\_{-i}, y\_{i}) = \frac{N\_{-nk}}{N-1+\alpha}\prod\_{d=1}^{D}\frac{(\beta + \sum\_{i\in C\_{k}} x\_{id})^{x\_{nd}}(\gamma + N\_{K} - \sum\_{i \in C\_{k}} x\_{id})^{1-x\_{nd}}}{\beta + \gamma + N\_{k}}$$

$$P(c\_{i} = c^{\*}\ |\ c\_{-i}, y\_{i}) = \frac{\alpha}{N-1+\alpha}\prod\_{d=1}^{D} \frac{B(x\_{nd} + \beta, \gamma - x\_{nd} +1)}{B(\beta, \gamma)}$$

Now note that these probabilities are all that is required to Gibbs
sample from the Dirichlet Process Mixture Model, as it has collapsed out
$π$ and $θ$ (although I am treating $α$ as a random variable distributed
according to a Gamma distribution). This is very attractive, and in
addition its relative simple abstract model has helped with its
popularity and is very quick to implement.

The output of my CRP sampler is shown below, and as seen I found that it
tended to start new clusters too readily, hence the large number of
clusters found here.

    samp_crp <- gibbs_dp(K3_N1000_P5, nsamples = 10000, maxK = 20, burnin = 1000, relabel = FALSE)
    plot_gibbs(samp_crp, cluster_threshold = 0.1)

![](/img/gibbs_20190925/unnamed-chunk-4-1.png)

Stick-breaking
--------------

The stick breaking formulation is another way of viewing a Dirichlet
Process. Remember the second model above, where the individual
parameters are viewed as draws from $G$, which is the sum of discrete
realisations,

$$\bar{\theta}\_i \sim G = \sum\_{k=1}^{K} \pi\_k \delta\_{\theta\_k}(\bar{\theta}\_i)$$

Then the weights $π$ can be seen as an iterative process of breaking a
unit length stick and seeing how much remains, where the breakpoints
$V$<sub>$i$</sub> are drawn from a Beta distribution with the
all-too-familiar $α$ as one of the parameters.

$$\pi\_i = V\_{i} \prod\_{j=1}^{i-1} (1 - V\_{j})$$

$$V\_1, \dots, V\_K \sim \text{Beta}(1, \alpha)$$

Section 5.2 in (Ishwaran and James
2001)(<https://www.tandfonline.com/doi/abs/10.1198/016214501750332758>)
provides the steps for Gibbs sampling from this method. Note that it is
not a collapsed sampler, all variables of interest are sampled here.
While the stick-breaking metaphor might be less easy to follow than the
CRP, it is a more flexible method as it can be easily calculated (given
a maximum value of K) and used in place of a Dirichlet distribution on
$π$ that finite mixture models use.

The output below from my C++ implementation of it shows similar results
to the CRP formulation, although there is noticeably less label swapping
and also less spikes of smaller groups. The reduced label switching is
actually undesired behaviour since this means the sampler isn’t
effectively exploring the space, this could be due to sampling $θ$ as
well. However, identifying fewer transient clusters is definitely a
bonus.

    samp_sb <- gibbs_stickbreaking(K3_N1000_P5, nsamples = 10000, maxK = 20, burnin = 1000, relabel = FALSE)

    plot_gibbs(samp_sb, cluster_threshold = 0.1)

![](/img/gibbs_20190925/unnamed-chunk-6-1.png)

Overall
-------

In conclusion, I hope this post has provided a useful practical
alternative to all the very theoretical guides to Dirichlet Processes
and shows how two of the most common formulations differ when it comes
to applying them in practice. Note that there has been considerable
research in effective sampling of Dirichlet Processes since 
Ishawaran and James (2001)
and Neal (2000), many of them adding jump operations in order to more
effectively traverse the problem space. However, these models are still
very challenging to effectively sample from in real applications where
data can be highly dimensional and multi-modal, see Hastie, Liverani,
and Richardson (2015) for a thorough review.

In further work I hope to find time to look into some of these methods,
particularly the Reversible Jump MCMC (Richardson and Green 1997) and
the Allocation Sampler (Nobile and Fearnside 2007). However, in my
day-to-day research, I do find implementing these models challenging as
they do not handle the large dimensionalities well and the label
switching can be a tough problem to fix.

References
----------

Hastie, David, Silvia Liverani, and Sylvia Richardson. 2015. “Sampling
from Dirichlet Process Mixture Models with Unknown Concentration
Parameter: Mixing Issues in Large Data Implementations.” $Statistics and
Computing$. <https://doi.org/10.1007/s11222-014-9471-3>.

Ishwaran, Hemant, and Lancelot James. 2001. “Gibbs Sampling Methods for
Stick-Breaking Priors.” *Journal of the American Statistical
ASsociation*. <https://doi.org/10.1198/016214501750332758>.

Neal, Radford. 2000. “Markov Chain Sampling Methods for Dirichlet
Process Mixture Models.” *Journal of Computational and Graphical
Statistics*. <https://doi.org/10.2307/1390653>.

Nobile, Agostino, and Alastair Fearnside. 2007. “Bayesian Finite Mixture
Models with an Unknown Number of Components the Allocation Sampler.”
*Statistics and Computing*. <https://doi.org/10.1007/s11222-006-9014-7>.

Richardson, Sylvia, and Peter Green. 1997. “On Bayesian Analysis of
Mixtures with an Unknown Number of Components.” *Journal of the Royal
Statistical Society Statistical Methodology Series B*.
<https://doi.org/10.1111/1467-9868.00095>.

Van Der Maaten, Laurens. n.d. “Bayesian Mixtures of Bernoulli
Distributions.”
